{"cells":[{"cell_type":"markdown","metadata":{"id":"pd8hfaKzkkiY"},"source":["# Im2Spec Perovskite IV"]},{"cell_type":"markdown","metadata":{"id":"Glvx9soVyj1f"},"source":["This notebook demonstrates a new version of im2spec model where one trains jointly image-to-image and spec-to-spec variational autoencoders (VAEs) while trying to 'align' their respective latent spaces. The latent space 'alignment' is done with two methods: *i)* using cosine similarity as an extra loss term, and *ii)* learning the linear transformation map between the two latent spaces during the model training. Here we also add rotational and (optionally) translational invariance to image-to-image VAE.\n","\n","We analyze hybrid perovskite IV dataset (Yongtao Liu and Jiawei Gong)"]},{"cell_type":"markdown","metadata":{"id":"Z-8n3K7SjPdl"},"source":["# Download:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEXrujx47ldC","outputId":"b91287e0-e8b3-478a-c93e-3335657f0b2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting atomai\n","  Downloading atomai-0.7.8-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from atomai) (2.4.1+cu121)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from atomai) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.10/dist-packages (from atomai) (3.7.1)\n","Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from atomai) (1.13.1)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from atomai) (1.5.2)\n","Requirement already satisfied: scikit-image>=0.16.2 in /usr/local/lib/python3.10/dist-packages (from atomai) (0.24.0)\n","Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from atomai) (4.10.0.84)\n","Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from atomai) (3.3)\n","Collecting mendeleev<=0.6.1 (from atomai)\n","  Downloading mendeleev-0.6.1.tar.gz (193 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torchvision>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from atomai) (0.19.1+cu121)\n","Requirement already satisfied: progressbar2>=3.38.0 in /usr/local/lib/python3.10/dist-packages (from atomai) (4.5.0)\n","Collecting gpytorch>=1.9.1 (from atomai)\n","  Downloading gpytorch-1.13-py3-none-any.whl.metadata (8.0 kB)\n","Collecting jaxtyping==0.2.19 (from gpytorch>=1.9.1->atomai)\n","  Downloading jaxtyping-0.2.19-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch>=1.9.1->atomai) (1.3.0)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mCollecting linear-operator>=0.5.3 (from gpytorch>=1.9.1->atomai)\n","Collecting sidpy\n","  Downloading sidpy-0.12.3-py2.py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from sidpy) (1.26.4)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from sidpy) (0.12.1)\n","Collecting cytoolz (from sidpy)\n","  Downloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from sidpy) (2024.8.0)\n","Requirement already satisfied: h5py>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from sidpy) (3.11.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sidpy) (3.7.1)\n","Requirement already satisfied: distributed>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sidpy) (2024.8.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from sidpy) (5.9.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sidpy) (1.16.0)\n","Requirement already satisfied: joblib>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from sidpy) (1.4.2)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from sidpy) (7.7.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from sidpy) (5.5.6)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from sidpy) (7.34.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sidpy) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sidpy) (1.13.1)\n","Collecting ase (from sidpy)\n","  Downloading ase-3.23.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting ipympl (from sidpy)\n","  Downloading ipympl-0.9.4-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (8.1.7)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (2.2.1)\n","Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (3.1.4)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (1.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (24.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (6.0.2)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (3.0.0)\n","Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (6.3.3)\n","Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (2.2.3)\n","Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.0.0->sidpy) (3.0.0)\n","Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->sidpy) (2024.6.1)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask->sidpy) (1.4.2)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->sidpy) (8.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->sidpy) (2.8.2)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->sidpy) (0.2.0)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->sidpy) (5.7.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->sidpy) (6.1.12)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (71.0.4)\n","Collecting jedi>=0.16 (from ipython->sidpy)\n","  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (3.0.48)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->sidpy) (4.9.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->sidpy) (3.6.9)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->sidpy) (3.0.13)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sidpy) (3.5.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->sidpy) (3.20.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->sidpy) (0.8.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2.0.0->sidpy) (2.1.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->sidpy) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->sidpy) (0.2.13)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->sidpy) (6.5.5)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->sidpy) (5.7.2)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->sidpy) (24.0.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->sidpy) (4.3.6)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (23.1.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (5.10.4)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (6.5.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.21.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.1.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.2.4)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.3.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (4.23.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (21.2.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.20.0)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.24.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (2.6)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (2.22)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.8.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->sidpy) (1.2.2)\n","Downloading sidpy-0.12.3-py2.py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ase-3.23.0-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipympl-0.9.4-py3-none-any.whl (516 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","Installing collected packages: jedi, cytoolz, ase, ipympl, sidpy\n"]}],"source":["!pip install atomai\n","!pip install sidpy"]},{"cell_type":"markdown","metadata":{"id":"eRhZU2cpx1Yu"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuCbFdpBMfo0"},"outputs":[],"source":["from typing import Union, Tuple, List, Type, Optional\n","\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from atomai import utils\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","\n","from scipy.signal import find_peaks\n","import h5py\n","import sidpy"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xue6hOQSNiFX"},"outputs":[],"source":["#@title convEncoderNet { form-width: \"20%\" }\n","\n","from warnings import warn, filterwarnings\n","filterwarnings(\"ignore\", module=\"torch.nn.functional\")\n","tt = torch.tensor\n","\n","########################### Convolutional neural nets #########################\n","\n","class convEncoderNet(nn.Module):\n","    \"\"\"\n","    Standard convolutional encoder\n","    \"\"\"\n","    def __init__(self,\n","                 input_dim: Tuple[int],\n","                 input_channels: int = 1,\n","                 latent_dim: int = 2,\n","                 hidden_dim: List[Tuple[int]] = None,\n","                 batchnorm: bool = True,\n","                 activation: str = \"lrelu\",\n","                 pool: bool = True,\n","                 ) -> None:\n","        \"\"\"\n","        Initializes encoder module\n","        \"\"\"\n","        super(convEncoderNet, self).__init__()\n","        if hidden_dim is None:\n","            hidden_dim = [(32,), (64, 64), (96, 96)]\n","        output_dim = [dim // 2**len(hidden_dim) for dim in input_dim]\n","        output_channels = hidden_dim[-1][-1]\n","        self.latent_dim = latent_dim\n","        self.feature_extractor = FeatureExtractor(\n","            len(input_dim), input_channels, hidden_dim,\n","            batchnorm, activation, pool)\n","        self.features2latent = features_to_latent(\n","            [output_channels, *output_dim], 2*latent_dim)\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        x = self.feature_extractor(x)\n","        encoded = self.features2latent(x)\n","        mu, log_sig = encoded.split(self.latent_dim, 1)\n","        return mu, log_sig\n","\n","\n","class convDecoderNet(nn.Module):\n","    \"\"\"\n","    Standard convolutional decoder\n","    \"\"\"\n","    def __init__(self,\n","                 latent_dim: int,\n","                 output_dim: int,\n","                 output_channels: int = 1,\n","                 hidden_dim: List[Tuple[int]] = None,\n","                 batchnorm: bool = True,\n","                 activation: str = \"lrelu\",\n","                 sigmoid_out: bool = True,\n","                 upsampling_mode: str = \"bilinear\",\n","                 ) -> None:\n","        \"\"\"\n","        Initializes decoder module\n","        \"\"\"\n","        super(convDecoderNet, self).__init__()\n","        if hidden_dim is None:\n","            hidden_dim = [(96, 96), (64, 64), (32,)]\n","        input_dim = [dim // 2**len(hidden_dim) for dim in output_dim]\n","        self.latent2features = latent_to_features(\n","            latent_dim, [hidden_dim[0][0], *input_dim])\n","        self.upsampler = Upsampler(\n","            len(output_dim), hidden_dim[0][0], hidden_dim, output_channels,\n","            batchnorm, activation, upsampling_mode)\n","        self.activation_out = nn.Sigmoid() if sigmoid_out else lambda x: x\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        x = self.latent2features(x)\n","        x = self.activation_out(self.upsampler(x))\n","        return x\n","\n","\n","class ConvBlock(nn.Module):\n","    \"\"\"\n","    Creates a block of layers each consisting of convolution operation,\n","    (optional) nonlinear activation and (optional) batch normalization\n","    \"\"\"\n","    def __init__(self,\n","                 ndim: int,\n","                 input_channels: int,\n","                 hidden_dim: Tuple[int],\n","                 kernel_size: Union[Tuple[int], int] = 3,\n","                 stride: Union[Tuple[int], int] = 1,\n","                 padding: Union[Tuple[int], int] = 1,\n","                 batchnorm: bool = False,\n","                 activation: str = \"lrelu\",\n","                 pool: bool = False,\n","                 ) -> None:\n","        \"\"\"\n","        Initializes module parameters\n","        \"\"\"\n","        super(ConvBlock, self).__init__()\n","        if not 0 < ndim < 4:\n","            raise AssertionError(\"ndim must be equal to 1, 2 or 3\")\n","        activation = get_activation(activation)\n","        hidden_dim = (input_channels,) + hidden_dim\n","        block = []\n","        for in_channels, out_channels in zip(hidden_dim, hidden_dim[1:]):\n","            block.append(get_conv(ndim)(in_channels, out_channels,\n","                         kernel_size=kernel_size, stride=stride, padding=padding))\n","            if activation is not None:\n","                block.append(activation())\n","            if batchnorm:\n","                block.append(get_bnorm(ndim)(out_channels))\n","        if pool:\n","            block.append(get_maxpool(ndim)(2, 2))\n","        self.block = nn.Sequential(*block)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Defines a forward pass\n","        \"\"\"\n","        output = self.block(x)\n","        return output\n","\n","\n","class UpsampleBlock(nn.Module):\n","    \"\"\"\n","    Upsampling performed using bilinear or nearest-neigbor interpolation\n","    followed by 1-by-1 convolution, which an be used to reduce a number of\n","    feature channels\n","    \"\"\"\n","    def __init__(self,\n","                 ndim: int,\n","                 input_channels: int,\n","                 output_channels: int,\n","                 scale_factor: int = 2,\n","                 mode: str = \"bilinear\") -> None:\n","        \"\"\"\n","        Initializes module parameters\n","        \"\"\"\n","        super(UpsampleBlock, self).__init__()\n","        warn_msg = (\"'bilinear' mode is not supported for 1D and 3D;\" +\n","                    \" switching to 'nearest' mode\")\n","        if mode not in (\"bilinear\", \"nearest\"):\n","            raise NotImplementedError(\n","                \"Use 'bilinear' or 'nearest' for upsampling mode\")\n","        if not 0 < ndim < 4:\n","            raise AssertionError(\"ndim must be equal to 1, 2 or 3\")\n","        if mode == \"bilinear\" and ndim in (3, 1):\n","            warn(warn_msg, category=UserWarning)\n","            mode = \"nearest\"\n","        self.mode = mode\n","        self.scale_factor = scale_factor\n","        self.conv = get_conv(ndim)(\n","            input_channels, output_channels,\n","            kernel_size=1, stride=1, padding=0)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Defines a forward pass\n","        \"\"\"\n","        x = F.interpolate(\n","            x, scale_factor=self.scale_factor, mode=self.mode)\n","        return self.conv(x)\n","\n","\n","class FeatureExtractor(nn.Sequential):\n","    \"\"\"\n","    Convolutional feature extractor\n","    \"\"\"\n","    def __init__(self,\n","                 ndim: int,\n","                 input_channels: int = 1,\n","                 hidden_dim: List[Tuple[int]] = None,\n","                 batchnorm: bool = True,\n","                 activation: str = \"lrelu\",\n","                 pool: bool = True,\n","                 ) -> None:\n","        \"\"\"\n","        Initializes feature extractor module\n","        \"\"\"\n","        super(FeatureExtractor, self).__init__()\n","        if hidden_dim is None:\n","            hidden_dim = [[16,], (32, 32), (64, 64)]\n","        for i, hdim in enumerate(hidden_dim):\n","            in_filters = input_channels if i == 0 else hidden_dim[i-1][-1]\n","            block = ConvBlock(ndim, in_filters, hdim,\n","                              batchnorm=batchnorm, activation=activation,\n","                              pool=pool)\n","            self.add_module(\"c{}\".format(i), block)\n","\n","\n","class Upsampler(nn.Sequential):\n","    \"\"\"\n","    Convolutional upsampler\n","    \"\"\"\n","    def __init__(self,\n","                 ndim: int,\n","                 input_channels: int,\n","                 hidden_dim: List[Tuple[int]] = None,\n","                 output_channels: int = 1,\n","                 batchnorm: bool = True,\n","                 activation: str = \"lrelu\",\n","                 upsampling_mode: str = \"bilinear\",\n","                 ) -> None:\n","        \"\"\"\n","        Initializes upsampler module\n","        \"\"\"\n","        super(Upsampler, self).__init__()\n","        if hidden_dim is None:\n","            hidden_dim = [(64, 64), (32, 32), (16,)]\n","        upsampling_mode = 'nearest' if ndim in (1, 3) else upsampling_mode\n","        for i, hdim in enumerate(hidden_dim):\n","            in_filters = input_channels if i == 0 else hidden_dim[i-1][-1]\n","            block = ConvBlock(ndim, in_filters, hdim,\n","                              batchnorm=batchnorm, activation=activation,\n","                              pool=False)\n","            self.add_module(\"conv_block_{}\".format(i), block)\n","            up = UpsampleBlock(ndim, hdim[-1], hdim[-1], mode=upsampling_mode)\n","            self.add_module(\"up_{}\".format(i), up)\n","\n","        out = ConvBlock(ndim, hdim[-1], (1,), 1, 1, 0, activation=None)\n","        self.add_module(\"output_layer\", out)\n","\n","\n","class features_to_latent(nn.Module):\n","    \"\"\"\n","    Maps features (usually, from a convolutional net/layer) to latent space\n","    \"\"\"\n","    def __init__(self, input_dim: Tuple[int], latent_dim: int = 2) -> None:\n","        super(features_to_latent, self).__init__()\n","        self.reshape_ = torch.prod(tt(input_dim))\n","        self.fc_latent = nn.Linear(self.reshape_, latent_dim)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.view(-1, self.reshape_)\n","        return self.fc_latent(x)\n","\n","\n","class latent_to_features(nn.Module):\n","    \"\"\"\n","    Maps latent vector to feature space\n","    \"\"\"\n","    def __init__(self, latent_dim: int, out_dim: Tuple[int]) -> None:\n","        super(latent_to_features, self).__init__()\n","        self.reshape_ = out_dim\n","        self.fc = nn.Linear(latent_dim, torch.prod(tt(out_dim)).item())\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.fc(x)\n","        return x.view(-1, *self.reshape_)\n","\n","def get_bnorm(dim: int) -> Type[nn.Module]:\n","    bn_dict = {1: nn.BatchNorm1d, 2: nn.BatchNorm2d, 3: nn.BatchNorm3d}\n","    return bn_dict[dim]\n","\n","\n","def get_conv(dim: int) -> Type[nn.Module]:\n","    conv_dict = {1: nn.Conv1d, 2: nn.Conv2d, 3: nn.Conv3d}\n","    return conv_dict[dim]\n","\n","\n","def get_maxpool(dim: int) -> Type[nn.Module]:\n","    conv_dict = {1: nn.MaxPool1d, 2: nn.MaxPool2d, 3: nn.MaxPool3d}\n","    return conv_dict[dim]\n","\n","\n","\n","########################## Fully-connected neural nets #########################\n","\n","class fcEncoderNet(nn.Module):\n","    \"\"\"\n","    Standard fully-connected encoder NN for VAE.\n","    The encoder outputs mean and standard evidation of the encoded distribution.\n","    \"\"\"\n","    def __init__(self,\n","                 in_dim: Tuple[int],\n","                 latent_dim: int = 2,\n","                 input_channels: int = 1,\n","                 hidden_dim: List[int] = None,\n","                 activation: str = 'tanh',\n","                 softplus_out: bool = False,\n","                 flat: bool = True\n","                 ) -> None:\n","        \"\"\"\n","        Initializes module\n","        \"\"\"\n","        super(fcEncoderNet, self).__init__()\n","        if len(in_dim) not in [1, 2]:\n","            raise ValueError(\"in_dim must be (h, w) or (l,)\")\n","        if hidden_dim is None:\n","            hidden_dim = [256, 256]\n","        self.in_dim = torch.prod(tt(in_dim)).item() * input_channels\n","\n","        self.fc_layers = make_fc_layers(\n","            self.in_dim, hidden_dim, activation)\n","        self.fc11 = nn.Linear(hidden_dim[-1], latent_dim)\n","        self.fc12 = nn.Linear(hidden_dim[-1], latent_dim)\n","        self.activation_out = nn.Softplus() if softplus_out else lambda x: x\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        x = x.view(-1, self.in_dim)\n","        x = self.fc_layers(x)\n","        mu = self.fc11(x)\n","        sigma = self.activation_out(self.fc12(x))\n","        return mu, sigma\n","\n","\n","class fcDecoderNet(nn.Module):\n","    \"\"\"\n","    Standard fully-connected decoder for VAE\n","    \"\"\"\n","    def __init__(self,\n","                 latent_dim: int,\n","                 output_dim: Tuple[int],\n","                 output_channels: int = 1,\n","                 hidden_dim: List[int] = None,\n","                 activation: str = 'tanh',\n","                 sigmoid_out: bool = True,\n","                 ) -> None:\n","        \"\"\"\n","        Initializes module\n","        \"\"\"\n","        super(fcDecoderNet, self).__init__()\n","        if len(output_dim) not in [1, 2]:\n","            raise ValueError(\"in_dim must be (h, w) or (l,)\")\n","        if hidden_dim is None:\n","            hidden_dim = [256, 256]\n","        self.reshape = (output_channels,) + (output_dim)\n","        hidden_out = torch.prod(tt(output_dim)).item() * output_channels\n","\n","        self.fc_layers = make_fc_layers(\n","            latent_dim, hidden_dim, activation)\n","        self.out = nn.Linear(hidden_dim[-1], hidden_out)\n","        self.activation_out = nn.Sigmoid() if sigmoid_out else lambda x: x\n","\n","    def forward(self, z: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        x = self.fc_layers(z)\n","        x = self.activation_out(self.out(x))\n","        return x.view(-1, *self.reshape)\n","\n","\n","class sDecoderNet(nn.Module):\n","    \"\"\"\n","    Spatial generator (decoder) network with fully-connected layers\n","    \"\"\"\n","    def __init__(self,\n","                 latent_dim: int,\n","                 output_dim: Tuple[int],\n","                 output_channels: int = 1,\n","                 hidden_dim: List[int] = None,\n","                 activation: str = 'tanh',\n","                 sigmoid_out: bool = True,\n","                 ) -> None:\n","        \"\"\"\n","        Initializes module\n","        \"\"\"\n","        super(sDecoderNet, self).__init__()\n","        if len(output_dim) not in [1, 2]:\n","            raise ValueError(\"in_dim must be (h, w) or (l,)\")\n","        if hidden_dim is None:\n","            hidden_dim = [256, 256]\n","        self.reshape = (output_channels,) + (output_dim)\n","        coord_dim = 1 if len(output_dim) < 2 else 2\n","\n","        self.coord_latent = coord_latent(\n","            latent_dim, hidden_dim[0], coord_dim)\n","        self.fc_layers = make_fc_layers(\n","            hidden_dim[0], hidden_dim, activation)\n","        self.out = nn.Linear(hidden_dim[-1], output_channels)\n","        self.activation_out = nn.Sigmoid() if sigmoid_out else lambda x: x\n","\n","    def forward(self, x_coord: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        x = self.coord_latent(x_coord, z)\n","        x = self.fc_layers(x)\n","        x = self.activation_out(self.out(x))\n","        return x.view(-1, *self.reshape)\n","\n","\n","class coord_latent(nn.Module):\n","    \"\"\"\n","    The \"spatial\" part of the iVAE's decoder that allows for translational\n","    and rotational invariance (based on https://arxiv.org/abs/1909.11663)\n","    \"\"\"\n","    def __init__(self,\n","                 latent_dim: int,\n","                 out_dim: int,\n","                 ndim: int = 2,\n","                 activation_out: bool = True) -> None:\n","        \"\"\"\n","        Initializes module\n","        \"\"\"\n","        super(coord_latent, self).__init__()\n","        self.fc_coord = nn.Linear(ndim, out_dim)\n","        self.fc_latent = nn.Linear(latent_dim, out_dim, bias=False)\n","        self.activation = nn.Tanh() if activation_out else None\n","\n","    def forward(self,\n","                x_coord: torch.Tensor,\n","                z: Tuple[torch.Tensor]) -> torch.Tensor:\n","        batch_dim, n = x_coord.size()[:2]\n","        x_coord = x_coord.reshape(batch_dim * n, -1)\n","        h_x = self.fc_coord(x_coord)\n","        h_x = h_x.reshape(batch_dim, n, -1)\n","        h_z = self.fc_latent(z)\n","\n","        h_z = h_z.view(-1, h_z.size(-1))\n","        h = h_x.add(h_z.unsqueeze(1))\n","        h = h.reshape(batch_dim * n, -1)\n","        if self.activation is not None:\n","            h = self.activation(h)\n","        return h\n","\n","\n","def make_fc_layers(in_dim: int,\n","                   hidden_dim: List[int] = 128,\n","                   activation: str = \"tanh\"\n","                   ) -> Type[nn.Module]:\n","    \"\"\"\n","    Generates a module with stacked fully-connected (aka dense) layers\n","    \"\"\"\n","    fc_layers = []\n","    hidden_dim = [in_dim] + hidden_dim\n","    for h_in, h_out in zip(hidden_dim, hidden_dim[1:]):\n","        fc_layers.extend(\n","            [nn.Linear(h_in, h_out),\n","             get_activation(activation)()]),\n","             #get_bnorm(1)(h_out)])\n","    fc_layers = nn.Sequential(*fc_layers)\n","    return fc_layers\n","\n","\n","########################### Other utility functions ##########################\n","\n","def grid2xy(X1: torch.Tensor, X2: torch.Tensor) -> torch.Tensor:\n","    X = torch.cat((X1[None], X2[None]), 0)\n","    d0, d1 = X.shape[0], X.shape[1] * X.shape[2]\n","    X = X.reshape(d0, d1).T\n","    return X\n","\n","\n","def imcoordgrid(im_dim: Tuple[int]) -> torch.Tensor:\n","    xx = torch.linspace(-1, 1, im_dim[0])\n","    yy = torch.linspace(1, -1, im_dim[1])\n","    x0, x1 = torch.meshgrid(xx, yy)\n","    return grid2xy(x0, x1)\n","\n","\n","def generate_grid(data_dim: Tuple[int]) -> torch.Tensor:\n","    \"\"\"Generates 1D or 2D grid of coordinates. Returns a torch tensor with two\n","    axes. If the input data_dim indicates only one dimensional data, then the\n","    output will be a 2d torch tensor artificially augmented along the last\n","    dimension, of shape [N, 1].\n","    Args:\n","        data_dim:\n","            Dimensions of the input data.\n","    Raises:\n","        NotImplementedError:\n","            If the dimension (length) of the provided data_dim is not equal to\n","            1 or 2.\n","    Returns:\n","        The grid (always 2d).\n","    \"\"\"\n","\n","    if len(data_dim) not in [1, 2]:\n","        raise NotImplementedError(\"Currently supports only 1D and 2D data\")\n","    if len(data_dim) == 1:\n","        return torch.linspace(-1, 1, data_dim[0])[:, None]\n","    return imcoordgrid(data_dim)\n","\n","\n","def transform_coordinates(coord: torch.Tensor,\n","                          phi: Union[torch.Tensor, float] = 0,\n","                          coord_dx: Union[torch.Tensor, float] = 0,\n","                          scale: Union[torch.Tensor, float] = 1.,\n","                          ) -> torch.Tensor:\n","    \"\"\"\n","    Rotation of 2D coordinates followed by scaling and translation.\n","    For 1D grid, there is only transaltion. Operates on batches.\n","    \"\"\"\n","    if coord.shape[-1] == 1:\n","        return coord + coord_dx\n","    coord = rotate_coordinates(coord, phi)\n","    return coord + coord_dx\n","\n","\n","def rotate_coordinates(coord: torch.Tensor,\n","                       phi: torch.Tensor,\n","                       ) -> torch.Tensor:\n","    \"\"\"\n","    Rotation of 2D coordinates. Operates on batches\n","    \"\"\"\n","    rotmat_r1 = torch.stack([torch.cos(phi), torch.sin(phi)], 1)\n","    rotmat_r2 = torch.stack([-torch.sin(phi), torch.cos(phi)], 1)\n","    rotmat = torch.stack([rotmat_r1, rotmat_r2], axis=1)\n","    coord = torch.bmm(coord, rotmat)\n","    return coord\n","\n","def get_activation(activation: int) -> Type[nn.Module]:\n","    if activation is None:\n","        return\n","    activations = {\"lrelu\": nn.LeakyReLU, \"tanh\": nn.Tanh,\n","                   \"softplus\": nn.Softplus, \"relu\": nn.ReLU,\n","                   \"gelu\": nn.GELU}\n","    return activations[activation]\n","\n","\n","def init_dataloader(*args: torch.Tensor,\n","                    random_sampler: bool = False,\n","                    shuffle: bool = True,\n","                    **kwargs: int\n","                    ) -> Type[torch.utils.data.DataLoader]:\n","    \"\"\"\n","    Returns initialized PyTorch dataloader, which is used by pyroVED's trainers.\n","    The inputs are torch Tensor objects containing training data and (optionally)\n","    labels.\n","\n","    Example:\n","    >>> # Load training data stored as numpy array\n","    >>> train_data = np.load(\"my_training_data.npy\")\n","    >>> # Transform numpy array to toech Tensor object\n","    >>> train_data = torch.from_numpy(train_data).float()\n","    >>> # Initialize dataloader\n","    >>> train_loader = init_dataloader(train_data)\n","    \"\"\"\n","    batch_size = kwargs.get(\"batch_size\", 100)\n","    tensor_set = torch.utils.data.dataset.TensorDataset(*args)\n","    if random_sampler:\n","        sampler = torch.utils.data.RandomSampler(tensor_set)\n","        data_loader = torch.utils.data.DataLoader(\n","            dataset=tensor_set, batch_size=batch_size, sampler=sampler)\n","    else:\n","        data_loader = torch.utils.data.DataLoader(\n","            dataset=tensor_set, batch_size=batch_size, shuffle=shuffle)\n","    return data_loader\n","\n","\n","def generate_latent_grid(d: int, **kwargs) -> torch.Tensor:\n","    \"\"\"\n","    Generates a grid of latent space coordinates\n","    \"\"\"\n","    dist = torch.distributions\n","    if isinstance(d, int):\n","        d = [d, d]\n","    z_coord = kwargs.get(\"z_coord\")\n","    if z_coord:\n","        z1, z2, z3, z4 = z_coord\n","        grid_x = torch.linspace(z2, z1, d[0])\n","        grid_y = torch.linspace(z3, z4, d[1])\n","    else:\n","        grid_x = dist.Normal(0, 1).icdf(torch.linspace(0.95, 0.05, d[0]))\n","        grid_y = dist.Normal(0, 1).icdf(torch.linspace(0.05, 0.95, d[1]))\n","    z = []\n","    for xi in grid_x:\n","        for yi in grid_y:\n","            z.append(tt([xi, yi]).float().unsqueeze(0))\n","    return torch.cat(z), (grid_x, grid_y)\n","\n","\n","def set_torch_seed(seed: int = 1) -> None:\n","    \"\"\"Sets all torch manual seeds\"\"\"\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"jYaboyfjDb30"},"source":["# Download data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unmgY_yW3lZ6"},"outputs":[],"source":["!pip install --upgrade gdown\n","!gdown -q https://drive.google.com/uc?id=1_llkLoA2SgcrT1NTvL04M5wHO7XxOpB6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"573HXtX04Bgm"},"outputs":[],"source":["# unzip\n","!unzip -j /content/IV_data.zip;"]},{"cell_type":"markdown","metadata":{"id":"U-zP0e3qyDdK"},"source":["### Load topography"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGCnGbJI4Xlv"},"outputs":[],"source":["topoh5 = h5py.File(\"topo_6um_0.hf5\")\n","sidpy.hdf_utils.print_tree(topoh5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6P6nfqh4gom"},"outputs":[],"source":["topo = topoh5['BE Channels/Channels/Channels'][1, :,:,0]*260\n","mask1 = np.zeros((384, 384))\n","for i in range (384):\n","    mask1 [:,i]=topo.mean(1)\n","topo = topo - mask1\n","topo = topo - topo.min()\n","\n","plt.figure(dpi = 300)\n","plt.imshow(topo, cmap = \"gray\")\n","plt.colorbar(label = \"Height (nm)\", shrink = 0.8)\n","plt.xticks([])\n","plt.yticks([])"]},{"cell_type":"markdown","metadata":{"id":"JZBdKrG7yIUR"},"source":["### Load IV coordinates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScAJMvMI_j3K"},"outputs":[],"source":["iv_coordinates = np.load(\"coordinates.npy\")\n","print(iv_coordinates.shape)\n","\n","plt.figure(dpi = 300)\n","plt.imshow(topo, cmap = 'gray')\n","plt.colorbar(label = \"Height (nm)\", shrink = 0.8)\n","plt.scatter(iv_coordinates[:,1], iv_coordinates[:,0], c ='r', s = 10)\n","plt.xticks([])\n","plt.yticks([])\n"]},{"cell_type":"markdown","metadata":{"id":"OLodxOQCyaXV"},"source":["### Plot one IV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KWAFtvm8PI5"},"outputs":[],"source":["ivh5 = h5py.File(\"IV_light_n2V_100_0.hf5\")\n","sidpy.hdf_utils.print_tree(ivh5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zoCVuHy8oeA"},"outputs":[],"source":["iv = ivh5['BE Channels/Channels/Channels'][0,0,]*100\n","iv = iv-iv.max()\n","vdc = ivh5['BEPS/vdc_waveform']\n","vdc = np.asarray(vdc)\n","plt.scatter(vdc, iv, c = np.arange(len(vdc)), cmap = 'jet', s = 10)\n"]},{"cell_type":"markdown","metadata":{"id":"0ZYUVk-AzC9K"},"source":["### Define a function to smooth IV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFCe-OXPCR3C"},"outputs":[],"source":["# smooth/denoise iv\n","from scipy.signal import savgol_filter\n","\n","def smooth(y):\n","  y_smooth = savgol_filter(y, 11, 3)\n","  return y_smooth\n","\n","iv_smooth = smooth(iv)\n","\n","plt.plot(vdc, iv_smooth)\n","plt.plot(vdc, iv)\n"]},{"cell_type":"markdown","metadata":{"id":"AcNW4xdRzFqp"},"source":["### Load all IV and smooth them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImVpCBXh8oaO"},"outputs":[],"source":["# load all iv data\n","ivdata = np.zeros((543, 128))  # 543 IV, length = 128\n","for i in range (543):\n","  ivh5 = h5py.File(\"IV_light_n2V_{}_0.hf5\".format(i))\n","  iv = ivh5['BE Channels/Channels/Channels'][0,0,]*100\n","  # iv = iv-iv.max()\n","  ivdata[i,:] = iv\n","\n","ivdata = ivdata - ivdata.min()\n","\n","# smooth iv data\n","ivdata_smooth = np.zeros((543, 128))\n","for i in range (543):\n","  ivdata_smooth[i,:] = smooth(ivdata[i,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_J8EEzQXvPtz"},"outputs":[],"source":["vdc = ivh5['BEPS/vdc_waveform']\n","vdc = np.asarray(vdc)\n","plt.figure(figsize = (3, 2.5), dpi = 300)\n","\n","plt.plot(vdc, ivdata_smooth[100,:]-ivdata_smooth[100,:].max(), c = \"r\")\n","plt.plot(vdc, ivdata_smooth[110,:]-ivdata_smooth[110,:].max(), c = \"b\")\n","plt.plot(vdc, ivdata_smooth[120,:]-ivdata_smooth[120,:].max(), c = \"g\")\n","\n","plt.yticks([0.0, -0.1, -0.2, -0.3])\n","plt.xticks([0.0, -1.0, -2.0])\n","\n","plt.xlabel (\"DC Voltage (V)\")\n","plt.ylabel (\"Current (nA)\")"]},{"cell_type":"markdown","metadata":{"id":"LYOtRxiVzL7T"},"source":["# Im2Spec Analysis"]},{"cell_type":"markdown","metadata":{"id":"z_883NkKzkLS"},"source":["### Extract image patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3QetljHEp5T"},"outputs":[],"source":["# Normalize all structure\n","norm_ = lambda x: (x - x.min()) / (x.max() - x.min())\n","\n","image = topo\n","\n","windowsize = 28  # patch size\n","img, com, _ = utils.extract_subimages(image, iv_coordinates, windowsize)\n","img = img[:,:,:,0]\n","\n","indices = np.asarray(com)\n","\n","img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WsOucQcFVc8"},"outputs":[],"source":["plotid = 100\n","f, (ax1, ax2) = plt.subplots(1,2, figsize = (15, 5))\n","ax1.imshow(image, cmap = 'gray')\n","ax1.scatter(com[plotid,1], com[plotid,0], s = 10)\n","\n","ax2.imshow(img[plotid,], cmap = 'gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7EW3JmsFO9-"},"outputs":[],"source":["# Find the index of used coordinate, so that we can select the corresponding IV\n","def find_same_item(arr1, arr2):\n","  for item in arr1:\n","    if item in arr2:\n","      return item\n","\n","def find_sub_coor_index(array, sub_array):\n","  idx = []\n","  for i in range(sub_array.shape[0]):\n","    idx_x = np.where(array[:,1] == sub_array[i,1])\n","    idx_y = np.where(array[:,0] == sub_array[i,0])\n","    idx.append(find_same_item(idx_x[0], idx_y[0]))\n","\n","  return np.asarray(idx, dtype = int)\n","\n","# find the sub index\n","sub_idx = find_sub_coor_index(iv_coordinates, com)\n","\n","# extract the corresponding iv\n","iv_train = ivdata_smooth[sub_idx,]\n","iv_train.shape"]},{"cell_type":"markdown","metadata":{"id":"BjQwBUNDMuCS"},"source":["Show corresponding image patches and spectra"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uz-pVGEKTctP"},"outputs":[],"source":["k1 = 15; k2 = 300; k3 = 220; k4 = -15\n","plt.figure(dpi = 300)\n","plt.imshow(topo, cmap = 'gray')\n","plt.colorbar(label = \"Height (nm)\", shrink = 0.8)\n","plt.scatter(iv_coordinates[k1,1], iv_coordinates[k1,0], c ='r', s = 30)\n","plt.scatter(iv_coordinates[k2,1], iv_coordinates[k2,0], c ='b', s = 30)\n","plt.scatter(iv_coordinates[k3,1], iv_coordinates[k3,0], c ='g', s = 30)\n","plt.scatter(iv_coordinates[k4,1], iv_coordinates[k4,0], c ='orange', s = 30)\n","plt.xticks([])\n","plt.yticks([])\n","\n","plt.figure(figsize = (4, 3), dpi = 300)\n","plt.plot(vdc, ivdata_smooth[k1,:]-ivdata_smooth[k1,:].max(), c = \"r\")\n","plt.plot(vdc, ivdata_smooth[k2,:]-ivdata_smooth[k2,:].max(), c = \"b\")\n","plt.plot(vdc, ivdata_smooth[k3,:]-ivdata_smooth[k3,:].max(), c = \"g\")\n","plt.plot(vdc, ivdata_smooth[k4,:]-ivdata_smooth[k4,:].max(), c = \"orange\")\n","\n","plt.yticks([0.0, -0.2, -0.4, -0.6])\n","plt.xticks([0.0, -1.0, -2.0])\n","\n","plt.ylim(-0.65, 0.05)\n","\n","plt.xlabel (\"DC Voltage (V)\")\n","plt.ylabel (\"Current (nA)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3Xr_FtBWaAz"},"outputs":[],"source":["hys = []; max_current = [];\n","for i in range (len(ivdata_smooth)):\n","  half_spec_length = int(ivdata_smooth[i,].shape[0]/2)\n","  hys_i = np.abs(ivdata_smooth[i,:half_spec_length].sum()-ivdata_smooth[i,half_spec_length:].sum())\n","  hys.append(hys_i)\n","  max_current.append(ivdata_smooth[i,].max())\n","hys = norm_(np.asarray(hys))\n","max_current = norm_(np.asarray(max_current))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9O9inKjX6df"},"outputs":[],"source":["plt.figure(dpi = 300)\n","plt.imshow(topo, cmap = 'gray')\n","plt.scatter(iv_coordinates[:,1], iv_coordinates[:,0], c =hys, cmap = \"jet\", s = 10, vmax = 0.2)\n","plt.xticks([])\n","plt.yticks([])\n","plt.colorbar(label = \"Normalized Hysteresis (a.u.)\", shrink = 0.8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2BUwkufYz8q"},"outputs":[],"source":["plt.figure(dpi = 300)\n","plt.imshow(topo, cmap = 'gray')\n","plt.scatter(iv_coordinates[:,1], iv_coordinates[:,0], c =max_current, cmap = \"jet\", s = 10)\n","plt.xticks([])\n","plt.yticks([])\n","plt.colorbar(label = \"Normalized Maximum Current (a.u.)\", shrink = 0.8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5FVIBnHb3Pi"},"outputs":[],"source":["k = [200, 300, 400]\n","for i in k:\n","  plt.figure(figsize = (4, 3), dpi = 300)\n","  plt.plot(vdc, ivdata_smooth[i,:]-ivdata_smooth[i,:].max(), c = \"b\")\n","  plt.xlabel (\"DC Voltage (V)\")\n","  plt.ylabel (\"Current (nA)\")\n","  plt.ylim(-0.55, 0.05)\n","  plt.yticks([0.0, -0.25, -0.5])\n","  plt.xticks([0.0, -1.0, -2.0])\n","  plt.show()\n","\n","for i in k:\n","  plt.figure( dpi = 300)\n","  plt.imshow(img[i,], cmap = 'gray')\n","  plt.yticks([])\n","  plt.xticks([])\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFV7fauGGt9a"},"outputs":[],"source":["k = 111\n","_, (ax0, ax1, ax2) = plt.subplots(1,3, figsize = (15, 5))\n","ax0.imshow(image, cmap = 'gray')\n","ax0.scatter(com[k,1], com[k,0], s = 20, c= 'r')\n","ax1.imshow(img[k,], cmap = 'gray')\n","ax2.plot(vdc, iv_train[k,])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JusnG3bvWLtX"},"outputs":[],"source":["# Normalize all structure\n","norm_ = lambda x: (x - x.min()) / (x.max() - x.min())\n","\n","for i in range (img.shape[0]):\n","  img[i,] = norm_(img[i,])"]},{"cell_type":"markdown","metadata":{"id":"D2cfbRFLLobU"},"source":["## Prepare torch data for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_NJ6JLm86zC"},"outputs":[],"source":["imgdata = img\n","indices = com\n","specdata = iv_train\n","full_img = image\n","\n","imgdata = torch.from_numpy(imgdata).float()\n","specdata = torch.from_numpy(specdata).float()\n","\n","#specdata = F.avg_pool1d(specdata[:, None], 8, 8).squeeze()  # reduce the spectral size via average pooling\n","\n","print(imgdata.shape, specdata.shape)"]},{"cell_type":"markdown","metadata":{"id":"eiPp-WRYX23O"},"source":["Now let's define the ImSpec base class. It consists of a 'constructor' ```__init__``` where we define our model parameters, methods for obtaining pdf-s (e.g., ```_log_normal```), computing different loss metrics (e.g. ```_compute_kld```), performing a reparametrization trick (```_reparameterize```), etc. These methods are going to be reused in all subsequent ImSpec models. Finally, there is a ```forward``` method, which defines a forward propagation of input data through our model. Here, the images will be passed through one VAE and the spectra are going to be passed through another VAE. The first VAE also enforces rotational and (optionally) translational invariance of the latent representation. At the moment, the two paths are not interacting. Note that our ```forward``` method also records all the computed losses."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"erltBaxADBlt"},"outputs":[],"source":["#@title imspec { form-width: \"20%\" }\n","class imspec(nn.Module):\n","\n","    def __init__(self,\n","                 indim1: Tuple[int, int], # assumed to be images\n","                 indim2: Tuple[int], # assumed to be spectra\n","                 latent_dim: int = 2,  # for now, both VAEs will have the same number of latent dims\n","                 coord: int = 0, # 1 for rotation, 2 for translations, 3 for rotation+translations\n","                 **kwargs: torch.Tensor) -> None:\n","        \"\"\"Initialize model parameters\"\"\"\n","        super(imspec, self).__init__()\n","        device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n","        # encoder and decoder for first dataset (e.g. structural images)\n","        self.encoder1 = fcEncoderNet(indim1, latent_dim + coord)\n","        dnet = sDecoderNet if coord > 0 else fcDecoderNet\n","        self.decoder1 = dnet(latent_dim, indim1)\n","        # encoder and decoder for second dataset (e.g. spectra)\n","        self.encoder2 = fcEncoderNet(indim2, latent_dim)\n","        self.decoder2 = fcDecoderNet(latent_dim, indim2)\n","        # Initialize grid and get 'prior' parameters for coordinate transformations\n","        self.grid = generate_grid(indim1).to(device)\n","        self.coord = coord\n","        self.phi_prior = kwargs.get(\"phi_prior\", tt(math.pi / 2)).to(device)\n","        self.dt_prior = kwargs.get(\"dt_prior\", tt(0.5)).to(device)\n","        # Place all model parameters on the appropriate device\n","        self.to(device)\n","        self.device = device\n","\n","    def _compute_kld(self,\n","                     z: torch.Tensor,\n","                     q_param: Tuple[torch.Tensor],\n","                     p_param: Optional[Tuple[torch.Tensor]] = None\n","                     ) -> torch.Tensor:\n","        \"\"\"\n","        Computes KL divergence term between two normal distributions\n","        or (if p_param = None) between normal and standard normal distributions\n","        \"\"\"\n","        qz = self._log_normal(z, *q_param)\n","        if p_param is None:\n","            pz = self._log_unit_normal(z)\n","        else:\n","            pz = self._log_normal(z, *p_param)\n","        return qz - pz\n","\n","    @classmethod\n","    def _reparameterize(cls,\n","                        z_mu: torch.Tensor,\n","                        z_logsig: torch.Tensor\n","                        ) -> torch.Tensor:\n","        \"\"\"Reparametrization trick\"\"\"\n","        batch_dim = z_mu.size(0)\n","        z_dim = z_mu.size(1)\n","        z_sig = torch.exp(z_logsig)\n","        eps = z_mu.new(batch_dim, z_dim).normal_()\n","        return z_mu + z_sig * eps\n","\n","    @classmethod\n","    def _log_normal(cls,\n","                   x: torch.Tensor,\n","                   mu: torch.Tensor,\n","                   log_sig: torch.Tensor\n","                   ) -> torch.Tensor:\n","        \"\"\"\n","        Computes log-pdf of a normal distribution\n","        \"\"\"\n","        log_pdf = (-0.5 * math.log(2 * math.pi) - log_sig -\n","                   (x - mu)**2 / (2 * torch.exp(log_sig)**2))\n","        return torch.sum(log_pdf, dim=-1)\n","\n","    @classmethod\n","    def _log_unit_normal(cls, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Computes log-pdf of a unit normal distribution\n","        \"\"\"\n","        log_pdf = -0.5 * (math.log(2 * math.pi) + x ** 2)\n","        return torch.sum(log_pdf, dim=-1)\n","\n","    @classmethod\n","    def _compute_re(cls,\n","                    x: torch.Tensor,\n","                    x_reconstr: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Computes binary cross-entropy reconstruction loss\"\"\"\n","        reconstr_loss = F.binary_cross_entropy(\n","            x_reconstr.flatten(1), x.flatten(1), reduction='none').sum(-1)\n","        return reconstr_loss\n","\n","    def _transform_coordinates(self,\n","                               grid: torch.Tensor,\n","                               t_vec: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Rotation and translation of the 2d coordinate grid\"\"\"\n","        phi = t_vec[:, 0]\n","        dt = t_vec[:, 1:].unsqueeze(1) if self.coord > 1 else 0\n","        grid = rotate_coordinates(grid, phi)\n","        return grid + self.dt_prior * dt\n","\n","    def forward(self, x1: torch.Tensor, x2: torch.Tensor=None) -> torch.Tensor:\n","        \"\"\"Forward Pass\"\"\"\n","\n","        self.likelihood = 0\n","        self.kl_divergence = 0\n","\n","        # First VAE\n","        # Encode data\n","        z_mu1, z_logsig1 = self.encoder1(x1)\n","        # Obtain latent vector throught he 'reparametrization trick'\n","        z_vec1 = self._reparameterize(z_mu1, z_logsig1)\n","        # Split latent vector into parts associated with coordinate transformations and data content\n","        t_vec1 = z_vec1[:, :self.coord]\n","        z_vec1 = z_vec1[:, self.coord:]\n","        # Compute KLD for the 'conventional' latent vector\n","        self.kl_divergence += self._compute_kld(\n","            z_vec1, (z_mu1[:, self.coord:], z_logsig1[:, self.coord:]))\n","        # Compute KLD for the latent angle (if any)\n","        if self.coord in [1, 3]:\n","            p_loc = t_vec1.new_zeros(t_vec1.size(0), 1)\n","            p_scale = t_vec1.new_ones(t_vec1.size(0), 1) * self.phi_prior\n","            self.kl_divergence += self._compute_kld(\n","                t_vec1[:, :1], (z_mu1[:, :1], z_logsig1[:, :1]), (p_loc, p_scale))\n","            z_mu1, z_logsig1 = z_mu1[:, 1:], z_logsig1[:, 1:]\n","            t_vec1 = t_vec1[:, 1:] if t_vec1.size(-1) > 1 else t_vec1\n","        # Compute KLD for the latent shift (if any)\n","        if self.coord in [2, 3]:\n","            self.kl_divergence += self._compute_kld(\n","                t_vec1, (z_mu1[:, :2], z_logsig1[:, :2])) # assume unit normal prior\n","        # Do the affine transformation for the coordinate grid (if any)\n","        decoder_input = (z_vec1,)\n","        if self.coord:\n","            grid = self.grid.expand(t_vec1.shape[0], *self.grid.shape)\n","            grid = self._transform_coordinates(grid, t_vec1)\n","            decoder_input = (grid,) + decoder_input\n","        # Pass latent data and transformed grid (if any) to decoder\n","        x_reconstr1 = self.decoder1(*decoder_input)\n","        # Compute reconstruction loss\n","        self.likelihood += -self._compute_re(x1, x_reconstr1)\n","\n","        # Second VAE (just vanilla VAE)\n","        z_mu2, z_logsig2 = self.encoder2(x2)\n","        z_vec2 = self._reparameterize(z_mu2, z_logsig2)\n","        self.kl_divergence += self._compute_kld(z_vec2, (z_mu2, z_logsig2))\n","        x_reconstr2 = self.decoder2(z_vec2)\n","        self.likelihood += -self._compute_re(x2, x_reconstr2)\n","\n","        return (z_vec1, x_reconstr1), (z_vec2, x_reconstr2)"]},{"cell_type":"markdown","metadata":{"id":"ys9TL64Ltdo6"},"source":["## Train two VAEs without enforcing similarity b/w latent spaces\n","Now let's proceed to model training. First, we simply train two VAEs (one for images, and another for spectra) without enforcing similarity between the latent spaces."]},{"cell_type":"markdown","metadata":{"id":"p19VqWl5HlRu"},"source":["Initialize dataloader:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RuaMW7uHlRw"},"outputs":[],"source":["imgdata = norm_(imgdata)\n","specdata = norm_(specdata)\n","\n","train_loader = init_dataloader(imgdata.unsqueeze(1), specdata.unsqueeze(1), batch_size=200)  # unsqueeze(1) adds a channel dimension of 1"]},{"cell_type":"markdown","metadata":{"id":"BfY4XX3DHlRx"},"source":["Initialize our model and an optimizer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJtnD7uRHlRx"},"outputs":[],"source":["img_dim = (imgdata.shape[-1], imgdata.shape[-1])\n","spec_dim = (specdata.shape[-1],)\n","\n","set_torch_seed(1)\n","model1 = imspec(img_dim, spec_dim, latent_dim=2, coord=3)\n","optimizer = torch.optim.Adam(model1.parameters(), lr=1e-4)"]},{"cell_type":"markdown","metadata":{"id":"zRo1OLyrHlRy"},"source":["Finaly, we train the model. It is usually convenient to define a function that performs a single training step and then call it from within the training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XmiPJPvQkl_"},"outputs":[],"source":["epochs = 40000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbI-uHzaHlRy"},"outputs":[],"source":["def train_step1(x1: torch.Tensor, x2: torch.Tensor,\n","               beta: float = 1.) -> float:\n","    \"\"\" Single train step\"\"\"\n","    model1.train()\n","    _ = model1(x1.to(model1.device), x2.to(model1.device))\n","    loss = -(model1.likelihood - beta * model1.kl_divergence).mean()  # compute losses\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    return loss.item()\n","\n","# training loop\n","train_loss = []\n","for e in range(epochs):\n","    train_loss_i = 0\n","    for (x1, x2) in train_loader:\n","        train_loss_i += train_step1(x1, x2)\n","    train_loss.append(train_loss_i / len(train_loader))\n","    print(\"Epoch {},  Train loss {}\".format(e+1, train_loss[-1]))"]},{"cell_type":"markdown","metadata":{"id":"R4RlJcvHKivb"},"source":["Encode data into the latent space(s) using the trained model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfXGxD2rKivj"},"outputs":[],"source":["# initialize test dataloader (here it using the same data as the training dataloader but without random shuffling)\n","test_loader = init_dataloader(imgdata.unsqueeze(1), specdata.unsqueeze(1), shuffle=False)\n","\n","model1.eval()  # switch to evaluation model (turn off batchnorm, etc.)\n","with torch.no_grad():  # we do not need to track gradients at the prediction stage\n","    z_mu1 = torch.cat([model1.encoder1(x.to(model1.device))[0].cpu() for (x,_) in test_loader])  # encoded image data\n","    z_mu2 = torch.cat([model1.encoder2(x.to(model1.device))[0].cpu() for (_,x) in test_loader])  # encoded spectral data\n","\n","plt.scatter(z_mu1[:, -1], z_mu1[:, -2], s=24, alpha=0.3, label=\"image data\")\n","plt.scatter(z_mu2[:, -1], z_mu2[:, -2], s=24, alpha=0.3, label=\"spectral data\")\n","plt.legend()"]},{"cell_type":"markdown","source":["Correlation analysis with two latents"],"metadata":{"id":"zBvZg9avqCLF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fwBLFfZOCsb"},"outputs":[],"source":["plt.scatter(z_mu1[:, -1], z_mu2[:, -1], s=24, alpha=0.3, label=\"2 vs 2\")\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXtIjNEFOLa5"},"outputs":[],"source":["plt.scatter(z_mu1[:, -2], z_mu2[:, -1], s=24, alpha=0.3, label=\"1 vs 2\")\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6Ak2U8EOPxZ"},"outputs":[],"source":["plt.scatter(z_mu1[:, -1], z_mu2[:, -2], s=24, alpha=0.3, label=\"2 vs 1\")\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OJkNESROTeQ"},"outputs":[],"source":["plt.scatter(z_mu1[:, -2], z_mu2[:, -2], s=24, alpha=0.3, label=\"1 vs 1\")\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"d-XhNPIdKivk"},"source":["Now let's view the learned latent manifold(s) by projecting them to the data space(s):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_xMXwDSKivk"},"outputs":[],"source":["d = 8  # grid size\n","z, (gridx, gridy) = generate_latent_grid(d)\n","coord_grid = model1.grid.expand(len(z), *model1.grid.shape)\n","\n","\n","with torch.no_grad():\n","    manifold1 = model1.decoder1(coord_grid, z.to(model1.device)).cpu()  # get the image data manifold\n","    manifold2 = model1.decoder2(z.to(model1.device)).cpu()  # get the spectral data manifold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpsHX7ukKivk"},"outputs":[],"source":["grid = torchvision.utils.make_grid(manifold1, nrow=d, padding=1, pad_value=0.8)[0]\n","plt.figure(figsize=(6,6), dpi=100)\n","plt.imshow(grid, cmap='gray')\n","plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmzjD6EuKivk"},"outputs":[],"source":["_, axes = plt.subplots(d, d, figsize=(8, 8), dpi = 100,\n","                       subplot_kw={'xticks': [], 'yticks': []},\n","                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","\n","for ax, y in zip(axes.flat, manifold2):\n","    ax.plot(vdc,y.squeeze())\n","    ax.set_ylim(0.5, 1)"]},{"cell_type":"markdown","metadata":{"id":"seAmE_gWE96g"},"source":["## Train two VAEs with enforced similarity b/w latent spaces"]},{"cell_type":"markdown","metadata":{"id":"HhRXQgcbHdo-"},"source":["Now, let's define an ```imspec``` class that learns a linear mapping (in a form of coefficients of a linear equation) between two latent spaces durign the training. The pass forward then consists of the following steps:\n","1. Pass $x_1$ through *encoder-1*. Get latent representation $z_1$. Compute KLD. Pass $z_1$ through *decoder-1*. Compute reconstruction loss.\n","2. Pass $x_2$ through *encoder-2*. Get latent representation $z_2$. Compute KLD (the latter is invariant under affine trasformation).\n","3. Get the $z_2' = Az_2$ transformation. Compute the $L_1(z_1, z_2')$ score.\n","4. Pass $z_2'$ through *decoder-2* and compute a reconstruction loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgvN4Wuno_DN","cellView":"form"},"outputs":[],"source":["#@title imspec3\n","class imspec3(imspec):\n","\n","    def __init__(self,\n","                 indim1: Tuple[int, int], # assumed to be images\n","                 indim2: Tuple[int], # assumed to be spectra\n","                 latent_dim: int = 2,  # for now, both VAEs will have the same number of latent dims\n","                 coord: int = 0,\n","                 **kwargs: torch.Tensor) -> None:\n","        # Initilize encoders and decoders\n","        args = (indim1, indim2, latent_dim, coord)\n","        super(imspec3, self).__init__(*args, **kwargs)\n","        # Define the (learnable) linear transformation coefficients\n","        A = torch.zeros(latent_dim, latent_dim).fill_diagonal_(1)\n","        A = A.to(self.device)\n","        self.A = nn.Parameter(A)\n","\n","    @classmethod\n","    def _compute_l1(cls,\n","                    x1: torch.Tensor,\n","                    x2: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Computes L1 score\"\"\"\n","        return (x1 -x2).abs().mean()\n","\n","    def linear_transform(self, z: torch.Tensor):\n","        \"\"\"Performs a linear transformation of the latent encoding\"\"\"\n","        return z @ self.A\n","\n","    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward Pass\"\"\"\n","\n","        self.likelihood = 0\n","        self.kl_divergence = 0\n","\n","        # First VAE\n","        # Encode data\n","        z_mu1, z_logsig1 = self.encoder1(x1)\n","        # Obtain latent vector throught he 'reparametrization trick'\n","        z_vec1 = self._reparameterize(z_mu1, z_logsig1)\n","        # Split latent vector into parts associated with coordinate transformations and data content\n","        t_vec1 = z_vec1[:, :self.coord]\n","        z_vec1 = z_vec1[:, self.coord:]\n","        # Compute KLD for the 'conventional' latent vector\n","        self.kl_divergence += self._compute_kld(\n","            z_vec1, (z_mu1[:, self.coord:], z_logsig1[:, self.coord:]))\n","        # Compute KLD for the latent angle (if any)\n","        if self.coord in [1, 3]:\n","            p_loc = t_vec1.new_zeros(t_vec1.size(0), 1)\n","            p_scale = t_vec1.new_ones(t_vec1.size(0), 1) * self.phi_prior\n","            self.kl_divergence += self._compute_kld(\n","                t_vec1[:, :1], (z_mu1[:, :1], z_logsig1[:, :1]), (p_loc, p_scale))\n","            z_mu1, z_logsig1 = z_mu1[:, 1:], z_logsig1[:, 1:]\n","            t_vec1 = t_vec1[:, 1:] if t_vec1.size(-1) > 1 else t_vec1\n","        # Compute KLD for the latent shift (if any)\n","        if self.coord in [2, 3]:\n","            self.kl_divergence += self._compute_kld(\n","                t_vec1, (z_mu1[:, :2], z_logsig1[:, :2])) # assume unit normal prior\n","        # Do the affine transformation for the coordinate grid (if any)\n","        decoder_input = (z_vec1,)\n","        if self.coord:\n","            grid = self.grid.expand(t_vec1.shape[0], *self.grid.shape)\n","            grid = self._transform_coordinates(grid, t_vec1)\n","            decoder_input = (grid,) + decoder_input\n","        # Pass latent data and transformed grid (if any) to decoder\n","        x_reconstr1 = self.decoder1(*decoder_input)\n","        # Compute reconstruction loss\n","        self.likelihood += -self._compute_re(x1, x_reconstr1)\n","\n","        # Second VAE\n","        z_mu2, z_logsig2 = self.encoder2(x2)\n","        z_mu2 = self.linear_transform(z_mu2) # Do linear transformation\n","\n","        z_vec2 = self._reparameterize(z_mu2, z_logsig2)\n","        self.kl_divergence +=  self._compute_kld(z_vec2, (z_mu2, z_logsig2))\n","\n","        self.simloss = self._compute_l1(z_vec1, z_vec2)  # Compute L1 score between two latent spaces\n","\n","        x_reconstr2 = self.decoder2(z_vec2)\n","        self.likelihood += -self._compute_re(x2, x_reconstr2)\n","\n","        return (z_vec1, x_reconstr1), (z_vec2, x_reconstr2)"]},{"cell_type":"markdown","metadata":{"id":"2RT5Y2GjCXnE"},"source":["Initialize, model, optimizer, and dataloader:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OX5QJ0HCXnQ"},"outputs":[],"source":["img_dim = (imgdata.shape[-1], imgdata.shape[-1])\n","spec_dim = (specdata.shape[-1],)\n","\n","# (re-)initialize dat loader\n","train_loader = init_dataloader(imgdata.unsqueeze(1), specdata.unsqueeze(1), batch_size=200)\n","\n","# initialize model and optimizer\n","set_torch_seed(1)\n","model = imspec3(img_dim, spec_dim, latent_dim=2, coord=3)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4 )"]},{"cell_type":"markdown","metadata":{"id":"TMe6X2J5FSoS"},"source":["Next we train our new model. Note that the main difference here is that we add a ```simloss``` term to the loss function. We also add a ```gamma``` coefficients before this new loss term which controls it's relative contribution to the total loss (similar to the idea of beta-VAE)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAtupijw3O5L"},"outputs":[],"source":["def train_step(x1: torch.Tensor, x2: torch.Tensor,\n","               beta: float = 1., gamma: float = 10.) -> float:\n","    model.train()\n","    _ = model(x1.to(model.device), x2.to(model.device))\n","    loss = -(model.likelihood - beta * model.kl_divergence).mean() + gamma * model.simloss\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    return loss.item(), model.simloss.item()\n","\n","train_loss, sim = [], []\n","for e in range(epochs):\n","    train_loss_i, sim_i = 0, 0\n","    for (x1, x2) in train_loader:\n","        loss_i, sscore_i = train_step(x1, x2)\n","        train_loss_i += loss_i\n","        sim_i += sscore_i\n","    train_loss.append(train_loss_i / len(train_loader))\n","    sim.append(sim_i / len(train_loader))\n","    print(\"Epoch {},  Train loss {}\".format(e+1, train_loss[-1]))"]},{"cell_type":"markdown","metadata":{"id":"KYxYG0UJFXWz"},"source":["Encode data into the latent spaces. Note that we have an extra line that applies a learned transformation to the encoded $z$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B81UoPxssumH"},"outputs":[],"source":["test_loader = init_dataloader(imgdata.unsqueeze(1), specdata.unsqueeze(1), shuffle=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    z_mu1 = torch.cat([model.encoder1(x.to(model.device))[0].cpu() for (x,_) in test_loader])\n","    z_mu2 = torch.cat([model.encoder2(x.to(model.device))[0].cpu() for (_,x) in test_loader])\n","    z_mu2 = z_mu2.matmul(model.A.cpu())  # a.matmul(b) is equivalent to a @ b and to torch.matmul(a, b)\n","\n","plt.figure (figsize = (5,5), dpi = 200)\n","plt.scatter(z_mu1[:, -1], z_mu1[:, -2], s=24, alpha=0.3, label=\"image data\")\n","plt.scatter(z_mu2[:, -1], z_mu2[:, -2], s=24, alpha=0.3, label=\"spectral data\")\n","plt.xlabel ('$Z$$_1$')\n","plt.ylabel ('$Z$$_2$')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"hMoFGRVesumI"},"source":["View the learned latent manifolds. Note that we have an extra line that applies a learned transformation to the $z$ vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bBvIqOIsumJ"},"outputs":[],"source":["d = 10\n","z, (gridx, gridy) = generate_latent_grid(d)\n","z_t = torch.matmul(z, model.A.cpu())\n","coord_grid = model.grid.expand(len(z), *model.grid.shape)\n","\n","with torch.no_grad():\n","    manifold1 = model.decoder1(coord_grid, z.to(model.device)).cpu()\n","    manifold2 = model.decoder2(z_t.to(model.device)).cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38w6TPsQsumK"},"outputs":[],"source":["grid = torchvision.utils.make_grid(manifold1, nrow=d, padding=1, pad_value=0.8)[0]\n","plt.figure(figsize=(6,6), dpi=200)\n","plt.imshow(grid, cmap='gray')\n","plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-riAiu39N2VI"},"outputs":[],"source":["_, axes = plt.subplots(d, d, figsize=(10, 10),dpi = 100,\n","                       subplot_kw={'xticks': [], 'yticks': []},\n","                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","\n","for ax, y in zip(axes.flat, manifold2):\n","    ax.plot(vdc, y.squeeze(), c = 'blue')\n","    ax.set_ylim(0.1, 1.2)"]},{"cell_type":"markdown","source":["Plot some example image-spectroscopy pairs"],"metadata":{"id":"mAocKnloqilW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvhiSnrQiWIx"},"outputs":[],"source":["plt.figure (figsize = (2,2), dpi =200)\n","plt.imshow(grid [59:59+28, 1:29], cmap = 'gray')\n","plt.xticks([])\n","plt.yticks([])\n","\n","k = 30\n","plt.figure(figsize = (2, 2), dpi = 300)\n","plt.plot(vdc, manifold2[k,0,]-manifold2[k,0,].max(), c = \"b\")\n","plt.yticks([0.0, -0.5, -1.0])\n","plt.xticks([0.0, -1.0, -2.0])\n","plt.ylim(-1.1, 0.1)\n","plt.xlabel (\"DC Voltage (V)\")\n","plt.ylabel (\"Normalized Current (a.u.)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVZDn8GrlULF"},"outputs":[],"source":["plt.figure (figsize = (2,2), dpi =200)\n","plt.imshow(grid [30:30+28, 146:146+28], cmap = 'gray')\n","plt.xticks([])\n","plt.yticks([])\n","\n","k = 15\n","plt.figure(figsize = (2, 2), dpi = 300)\n","plt.plot(vdc, manifold2[k,0,]-manifold2[k,0,].max(), c = \"b\")\n","plt.yticks([])\n","plt.xticks([0.0, -1.0, -2.0])\n","plt.ylim(-1.1, 0.1)\n","plt.xlabel (\"DC Voltage (V)\")\n","# plt.ylabel (\"Normalized Current (a.u.)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9OWvtx4mehr"},"outputs":[],"source":["plt.figure (figsize = (2,2), dpi =200)\n","plt.imshow(grid [175-29:175-29+28, 175+29:175+29+28], cmap = 'gray')\n","plt.xticks([])\n","plt.yticks([])\n","\n","k = 67\n","plt.figure(figsize = (2, 2), dpi = 300)\n","plt.plot(vdc, manifold2[k,0,]-manifold2[k,0,].max(), c = \"b\")\n","plt.yticks([])\n","plt.xticks([0.0, -1.0, -2.0])\n","plt.ylim(-1.1, 0.1)\n","plt.xlabel (\"DC Voltage (V)\")\n","# plt.ylabel (\"Normalized Current (a.u.)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEszydBnpG5Y"},"outputs":[],"source":["plt.figure (figsize = (2,2), dpi =200)\n","plt.imshow(grid [-29:-1, 175-29:175-29+28], cmap = 'gray')\n","plt.xticks([])\n","plt.yticks([])\n","\n","k = 95\n","plt.figure(figsize = (2, 2), dpi = 300)\n","plt.plot(vdc, manifold2[k,0,]-manifold2[k,0,].max(), c = \"b\")\n","plt.yticks([])\n","plt.xticks([0.0, -1.0, -2.0])\n","plt.ylim(-1.1, 0.1)\n","plt.xlabel (\"DC Voltage (V)\")"]},{"cell_type":"markdown","metadata":{"id":"wXwxBJwi6j7S"},"source":["Let's also define a function that takes images (spectra) as an input at outputs spectra (images) and try it on the image -> spectrum predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"588a84tJ6j7T"},"outputs":[],"source":["#@title hybrid_fwd\n","def hybrid_fwd(model: Type[torch.nn.Module], x: torch.Tensor,\n","               out: str) -> torch.Tensor:\n","    \"\"\"\n","    Passes the input features through the encoder of the first (second) model to obtain\n","    a latent code which is then passed through the decoder of the second (first) model\n","    \"\"\"\n","    if out not in ['image', 'spectrum']:\n","        raise NotImplementedError(\"argument 'out' must be 'image' or 'spectrum'\")\n","    x = x.to(model.device)\n","    model.eval()\n","    with torch.no_grad():\n","        z_mu = model.encoder2(x)[0] @ model.A if out == 'image' else model.encoder1(x)[0]\n","        z_mu = z_mu[:, model.coord:] if out == 'spectrum' else z_mu\n","        dec1_input = (z_mu,)\n","        if model.coord > 0:\n","            grid = model.grid.expand(z_mu.size(0), *model.grid.size())\n","            dec1_input = (grid,) + dec1_input\n","        x_rec = model.decoder1(*dec1_input) if out == 'image' else model.decoder2(z_mu)\n","    return x_rec.cpu()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENuB-TrlPDNk"},"outputs":[],"source":["for i in range (20):\n","  k = np.random.choice(len(specdata))\n","\n","  x_input = imgdata[k]\n","  x_input = x_input[None, None] # add two pseudo-dimensions for batch size and channel\n","  x_pred = hybrid_fwd(model, x_input, out='spectrum')\n","\n","  _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4), dpi = 100)\n","  ax1.imshow(x_input.squeeze())\n","  ax1.axis(\"off\")\n","  ax1.set_title(\"Structure\")\n","  ax2.plot(vdc, x_pred.squeeze(), label = \"prediction\")\n","  ax2.set_title(\"Spectrum--Prediction\")\n","  ax2.set_xlabel(\"Voltage (V)\")\n","  ax2.set_ylabel(\"Current (norm.)\")\n","  #ax2.set_ylim(0.25, 0.35)\n","\n","  ax3.set_title(\"Spectrum--Ground truth\")\n","  ax3.set_xlabel(\"Voltage (V)\")\n","  ax3.set_ylabel(\"Current (norm.)\")\n","  #ax3.set_ylim(0.25, 0.35)\n","  ax3.plot(vdc, specdata[k].squeeze(), color = \"darkorange\")\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"O7irQR48_40E"},"source":["We can also try predicting image from spectra."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXw7i7GY_40E"},"outputs":[],"source":["for i in range (20):\n","  k = np.random.choice(len(specdata))\n","  x_input = specdata[k]\n","  x_input = x_input[None, None] # add two pseudo-dimensions for batch size and channel\n","  x_pred = hybrid_fwd(model, x_input, out='image')\n","\n","  _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4), dpi = 100)\n","  ax2.imshow(x_pred.squeeze())\n","  ax2.axis(\"off\")\n","  ax2.set_title(\"Structure--Prediction\")\n","\n","  ax3.imshow(imgdata[k].squeeze())\n","  ax3.axis(\"off\")\n","  ax3.set_title(\"Structure--Ground truth\")\n","\n","  ax1.plot(vdc,x_input.squeeze())\n","  ax1.set_xlabel(\"Voltage (V)\")\n","  ax1.set_ylabel(\"Current (Normalized)\")\n","  ax1.set_title(\"Spectrum\")\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWu42fArbWDY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1psv2WgnZ7MkLYw_I1Xab9S3hF9HnkAjj","timestamp":1720128716060},{"file_id":"1a8LKk61FMMJF3zM6g27OVeKSGeJx8OPU","timestamp":1658017454741},{"file_id":"1g-yInKstxrXygXfJ8kfEUrbYM3Nr9bHO","timestamp":1657571146431},{"file_id":"1k2M0BuoIgroLRQB23Aq9ZXa_LX-O5csk","timestamp":1656341186752},{"file_id":"10h1kttYY7CrmlneN_jHq2U0Y7lzKQ73B","timestamp":1631828776048},{"file_id":"1cBB4Ct9hHDokJGGeLFdw73lrO6jSphka","timestamp":1631510459968},{"file_id":"1YVMinI2f1TPLYSm2Ou-6TVeVKFvtLvER","timestamp":1631403376671},{"file_id":"1zLSLnu8Z4YkfhhHaElJ3yW9wwfK8DgZj","timestamp":1631120585028}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}